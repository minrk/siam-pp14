{
 "metadata": {
  "name": "Downloading images from flickr"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook automates downloading a bunch of images from flickr,\n",
      "for use in the facial detection notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys,os,re,time\n",
      "import urllib\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from IPython import parallel"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rc = parallel.Client()\n",
      "all_engines = rc[:]\n",
      "view = rc.load_balanced_view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start with remote imports"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%px import urllib, sys, os, re, time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define our downloading functions on the engines\n",
      "\n",
      "Th flickr parsing code is adapted from [here](http://megasnippets.com/source-codes/python/get_random_interesting_image_flickr)\n",
      "\n",
      "The search specifically includes only Creative Commons licensed images with Safe Search on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "def extract_urls(html):\n",
      "    \"\"\"Extract images URLs from a page.\"\"\"\n",
      "    re_imageurl = re.compile(r'src=\"(http://farm\\d+.static.?flickr.com/\\d+/\\d+_\\w+_t.jpg)\"',re.IGNORECASE|re.DOTALL)\n",
      "    urls = re_imageurl.findall(html)\n",
      "    if len(urls)==0:\n",
      "        return []\n",
      "    urls = [url.replace('_t.jpg','_m.jpg') for url in urls]\n",
      "    return urls\n",
      "\n",
      "def urls_for_tag(tag='face', min_images=100, max_pages=20):\n",
      "    \"\"\"get urls to flickr images with given tag(s)\n",
      "\n",
      "    scrapes flickr search page\n",
      "    \"\"\"\n",
      "    urls = []\n",
      "    page = 1\n",
      "    while len(urls) < min_images and page <= max_pages:\n",
      "        url = 'http://www.flickr.com/search/?q=%s&l=cc&ss=0&ct=0&mt=photos&w=all&adv=1&m=tags&page=%i' % (tag, page)\n",
      "        print \"fetching %s\" % url\n",
      "        urlfile = urllib.urlopen(url)\n",
      "        html= urlfile.read()\n",
      "        urlfile.close()\n",
      "        page_urls = extract_urls(html)\n",
      "        urls.extend(page_urls)\n",
      "        print \"found %i images\" % len(urls)\n",
      "        page += 1\n",
      "        \n",
      "    return urls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specify the tags we want to download"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tags = ['face', 'boy', 'girl', 'portrait']\n",
      "all_engines.scatter('tags', tags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%px tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want a few hundred images per tag"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_engines['n'] = 100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load up the index of image URLs for each tag"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "\n",
      "urls = {}\n",
      "for tag in tags:\n",
      "    urls[tag] = urls_for_tag(tag, n)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each engine now has a dictionary of urls for tagged images.\n",
      "We can fetch all of the dictionaries and merge them into one big local dict."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url_dict = {}\n",
      "for d in all_engines['urls']:\n",
      "    url_dict.update(d)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the function for downloading an image from a url"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def download_image(url, dest_dir='images'):\n",
      "    \"\"\"download an image from a url into a directory\n",
      "\n",
      "    returns the path to the downloaded image.\n",
      "    \"\"\"\n",
      "    basename = url.rsplit('/', 1)[-1]\n",
      "    dest = os.path.join(dest_dir, basename)\n",
      "    if not os.path.exists(dest_dir):\n",
      "        os.makedirs(dest_dir)\n",
      "    if os.path.exists(dest):\n",
      "        print \"already have %s\" % dest\n",
      "        return dest\n",
      "    \n",
      "    print \"downloading %s -> %s\" % (url, dest)\n",
      "    urlf = urllib.urlopen(url)\n",
      "    data = urlf.read()\n",
      "    urlf.close()\n",
      "    with open(dest, 'w') as f:\n",
      "        f.write(data)\n",
      "    return dest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We turn our dict of urls into a dict of tasks, for downloading all of the images in parallel"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tasks = {}\n",
      "for tag, urls in url_dict.items():\n",
      "    tasks[tag] = view.map_async(download_image, urls, ['images/%s' % tag]*len(urls))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now a trick for waiting on a bunch of tasks.\n",
      "You can create a big AsyncResult from many other AsyncResults,\n",
      "and wait on it interactively:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_msg_ids = []\n",
      "for tag, amr in tasks.items():\n",
      "    all_msg_ids.extend(amr.msg_ids)\n",
      "flat_results = rc.get_result(all_msg_ids)\n",
      "flat_results.wait_interactive()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}